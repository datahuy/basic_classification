import re

import visen
from unicodedata import normalize
from pyvi import ViTokenizer

### brackets list
opening_ls = ['[', '{', 'â…', 'âŒ©', 'â¡', 'â¢', 'â£', 'â§', 'â¨', 'â©', 'â¬', 'â°', 'â²', 'â´', 'âŸ¦', 'âŸ¨', 'âŸª', 'âŸ¬', 'â¦ƒ', 'â¦‡', 'â¦‰',
              'â¦‹', 'â¦', 'â¦', 'â¦‘', 'â¦“', 'â¦•', 'â¦—', 'â§¼', 'â¸‚', 'â¸„', 'â¸‰', 'â¸Œ', 'â¸œ', 'â¸¢', 'â¸¤', 'â¸¦', 'ã€ˆ', 'ã€Š', 'ã€Œ', 'ã€',
              'ã€', 'ã€”', 'ã€–', 'ã€˜', 'ã€š', 'ï¹›', 'ï¹', 'ï¼»', 'ï½›', 'ï½¢', 'ï½£']

closing_ls = [']', '}', 'â†', 'âŒª', 'â¤', 'â¥', 'â¦', 'â«', 'â¬', 'â­', 'â­', 'â±', 'â³', 'âµ', 'âŸ§', 'âŸ©', 'âŸ«', 'âŸ­', 'â¦„', 'â¦ˆ', 'â¦Š',
              'â¦Œ', 'â¦', 'â¦', 'â¦’', 'â¦”', 'â¦–', 'â¦˜', 'â§½', 'â¸ƒ', 'â¸…', 'â¸Š', 'â¸', 'â¸', 'â¸£', 'â¸¥', 'â¸§', 'ã€‰', 'ã€‹', 'ã€', 'ã€',
              'ã€‘', 'ã€•', 'ã€—', 'ã€™', 'ã€›', 'ï¹œ', 'ï¹', 'ï¼½', 'ï½', 'ï½£']

opening_brackets = {key: '(' for key in opening_ls}
closing_brackets = {key: ')' for key in closing_ls}

### constant
PUNC = '!\"#$&()*+,-â€“âˆ’./:;=?@[\]^_`{|}~â€â€œ`Â°Â²Ëˆâ€ã„§â€›âˆ¼â€™'  # remove <> for number_sym and unknown_sym
UNKNOWN_SYM = ''  # mix number with character, ex: 6.23A

re_email = '([a-zA-Z0-9._%+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z]{2,4}(\\.?[a-zA-Z]{2,4})?)'
re_url = '(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9]+\.[^\s]{2,}|www\.[a-zA-Z0-9]+\.[^\s]{2,})'
re_url2 = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
re_image = '(táº­p[\s_]tin|hÃ¬nh|file|image|imagesize).*?(jpg|svg|png|gif|jpeg|ogg|tif|width)'
re_num_and_decimal = '[0-9]*[,.\-]*[0-9]*[,.\-]*[0-9]*[.,\-]*[0-9]*[,.\-]*[0-9]+[.,]?'
re_unknown = '[a-z]+[\d]+[\w]*|[\d]+[a-z]+[\w]*'
re_vnese_txt = r'[^a-zA-ZÃ Ã¡Ã£áº¡áº£Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº¹áº»áº½Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­Ä©á»‰á»‹Ã²Ã³Ãµá»á»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹ÃºÅ©á»¥á»§Æ°á»©á»«á»­á»¯á»±á»³á»µá»·á»¹Ã½Ã€ÃÃƒáº áº¢Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃÄ¨á»ˆá»ŠÃ’Ã“Ã•á»Œá»Ã”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢Ã™ÃšÅ¨á»¤á»¦Æ¯á»¨á»ªá»¬á»®á»°á»²á»´á»¶á»¸Ã\s]'
re_truncate_unknown = '(<UNKNUM>\s*)+'


special_punc = {'â€': '"', '': '', "â€™": "'", "`": "'"}


def replace_all(replacer: dict, txt: str) -> str:
    for old, new in replacer.items():
        txt = txt.replace(old, new)
    return txt


def replace_num(text: str) -> str:
    text = re.sub(re_num_and_decimal, UNKNOWN_SYM, text)
    return text


def replace_unknown(text: str) -> str:
    text = re.sub(re_unknown, UNKNOWN_SYM, text)
    return text


def unicode_normalizer(text, forms: list = ['NFKC', 'NKFD', 'NFC', 'NFD']) -> str:
    for form in forms:
        text = normalize(form, text)
    return text


def normalize_bracket(text: str) -> str:
    text = replace_all(opening_brackets, text)
    text = replace_all(closing_brackets, text)
    text = re.sub(r"[\(\[].*?[\)\]]", " ", text)
    return text


def remove_punc(text: str) -> str:
    r = re.compile(r'[\s{}]+'.format(re.escape(PUNC)))
    text = r.split(text)
    return ' '.join(i for i in text if i)


def truncate_unknown(text: str) -> str:
    text = re.sub(re_truncate_unknown, UNKNOWN_SYM, text)
    return text


with open('data/stop_words.txt', 'r') as fi:
    stop_words = fi.readlines() 
    stop_words = [word.rstrip() for word in stop_words]


def clean_text(text: str, stop_words=stop_words) -> str:
    text = str(text)
    text = text.split('\n')[0]
    text = unicode_normalizer(text, ["NFKC"])
    text = text.lower()
    text = remove_punc(text)
    text = truncate_unknown(text)
    text = re.sub(re_vnese_txt, " ", text)
    text = text.strip()
    text = ViTokenizer.tokenize(text)
    text = ' '.join([w for w in text.split() if w not in stop_words])
    return visen.clean_tone(text)


if __name__ == '__main__':
    txt = ['{DATE 5/2023} Sá»®A NAN NGA 1 1.800G (6LON/THÃ™NG)*ÄI ÄÆ N KHI Äá»¦ THÃ™NG* (Há»˜P) ğŸ˜‚â¤ğŸ˜ğŸ˜’ğŸ˜¢ğŸ˜‰ğŸ˜œğŸ‰',
           'S{PFGdjgjfghfjjh',
           '{Tem phá»¥} Máº·t náº¡ giáº¥y dÆ°á»¡ng da cao cáº¥p Whisis HÃ n Quá»‘c 25ml',
           'NÆ°á»›c táº©y trang innisfree trÃ  xanh máº«u má»›i ( KhÃ´ng Bill) [ Date 30/6/2024',
           '[Sá»¯a Táº¯m SÃ¡ng Da Reihaku Hatomugi 800ml',
           '[K.V[ DHA Bio Island Cho BÃ  Báº§u 60 ViÃªn( Máº«u má»›i) 123 456 789 asa4asfa535 gekko',
           'Máº·t Náº¡ Äáº¥t SÃ©t Báº¡c HÃ  Dreamworks I\'m The Real Shrek Pack 110g - 123 456 789 asa4asfa535',
           'NORMADERM PHYTOSOLUTION DOUBLE- CORRECTION DAILY CARE \"KEM DÆ¯á» NG Dáº NG GEL Sá»®A DÃ€NH CHO DA Má»¤N Vá»šI TÃC Äá»˜NG KÃ‰P GIÃšP GIáº¢M Má»¤N, GIáº¢M KHUYáº¾T ÄIá»‚M TRÃŠN DA & GIÃšP PHá»¤C Há»’I, DÆ¯á» NG áº¨M LÃ€N DA\"',
           '24Hr CREAM DEODORANT SENSITIVE OR DEPILATED SKIN \"KEM KHá»¬ MÃ™I VÃ€ GIÃšP DÆ¯á» NG DA Má»€M Má»ŠN (CHO LÃ€N DA NHáº Y Cáº¢M)\"',
           "Táº©y táº¿ bÃ o cháº¿t cho da máº·t \"Máº­t ong vÃ  Cafe\", Anteka 75ml anteka",
           'QUáº¦N JEAN LOUIS VUITTON T20T128',
           'ğ“—ğ“¾ğ”‚Ì€ğ“·ğ“± ğ“›ğ“ªÌ‚ğ“¶ ğ“£ğ“¸Ì‚Ì ğ“ğ“°ğ“ªÌ‚ğ“·',
           'sá»¯a i\'m ensure']

    for i in txt:
        print(clean_text(i))



    
